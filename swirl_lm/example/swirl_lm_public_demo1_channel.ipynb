{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hFspV_AIBs9"
      },
      "source": [
        "# Example 1: A Laminar Channel Flow\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/swirl-lm/blob/main/swirl_lm/example/swirl_lm_public_demo1_channel.ipynb)\n",
        "\n",
        "\n",
        "The example in this colab shows how to run a laminar channel flow with Swirl-LM on TPU. It shows how libraries in Swirl-LM is loaded and build, and the key\n",
        "components to set up a simulation with Swirl-LM.\n",
        "\n",
        "Note that this colab requires connection to a runtime with TPU. Before you run this Colab notebook, make sure that your hardware accelerator is a TPU by checking your notebook settings: **Runtime** \u003e **Change runtime type** \u003e **Hardware accelerator** \u003e **TPU**. The default TPU runtime has 8 cores available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRksQQC3QSy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-research/swirl-lm.git\n",
        "!./swirl-lm/swirl_lm/setup.sh\n",
        "!python3 -m pip show swirl-lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1clY_Anlsoeo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from absl import flags\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from swirl_lm.base import driver\n",
        "from swirl_lm.base import driver_tpu\n",
        "from swirl_lm.base import parameters\n",
        "from swirl_lm.utility import get_kernel_fn\n",
        "from swirl_lm.utility import tpu_util\n",
        "\n",
        "flags.FLAGS(sys.argv[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SUgbnibHvf1"
      },
      "source": [
        "# Simulation Setup\n",
        "To initialize a simulation, one needs to provide the following:\n",
        "   * A text protobuf that specifies the simulation configuration (e.g. domain\n",
        "     size, partitions) and user defined parameters for a simulation\n",
        "   * A text protobuf file that specifies the solver parameters and physical\n",
        "     conditions of a simulation, e.g. boundary conditions\n",
        "   * A function that initializes state variables to start the simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q68e7H10Ek_"
      },
      "outputs": [],
      "source": [
        "# Simulation configuration.\n",
        "\n",
        "params = parameters.SwirlLMParameters.config_from_text_proto(\n",
        "    \"\"\"\n",
        "  # proto-file: swirl_lm/base/parameters.proto\n",
        "  # proto-message: SwirlLMParameters\n",
        "\n",
        "  solver_procedure: VARIABLE_DENSITY\n",
        "  convection_scheme: CONVECTION_SCHEME_QUICK\n",
        "  time_integration_scheme: TIME_SCHEME_CN_EXPLICIT_ITERATION\n",
        "  grid_params {\n",
        "    # The number of cores in 3 dimensions.\n",
        "      computation_shape {\n",
        "        dim_0: 2\n",
        "        dim_1: 2\n",
        "        dim_2: 1\n",
        "      }\n",
        "    # The physical size of the simulation domain in units of m.\n",
        "    length {\n",
        "      dim_0: 4.0\n",
        "      dim_1: 1.0\n",
        "      dim_2: 0.01\n",
        "    }\n",
        "    # The number of grid points per core in 3 dimensions including ghost cells\n",
        "    # (halos).\n",
        "    grid_size {\n",
        "      dim_0: 128\n",
        "      dim_1: 64\n",
        "      dim_2: 8\n",
        "    }\n",
        "    # The width of the ghost cells on each side of the domain. It is set to 2\n",
        "    # considering the stencil width of the QUICK scheme.\n",
        "    halo_width: 2\n",
        "    # The time step size in units of s.\n",
        "    dt: 0.001\n",
        "    # The size of the convolution kernel to be used for fundamental numerical\n",
        "    # operations.\n",
        "    kernel_size: 16\n",
        "    periodic {\n",
        "      dim_0: false\n",
        "      dim_1: false\n",
        "      dim_2: true\n",
        "    }\n",
        "  }\n",
        "  pressure {\n",
        "    solver {\n",
        "      jacobi {\n",
        "        max_iterations: 10 halo_width: 2 omega: 0.67\n",
        "      }\n",
        "    }\n",
        "    num_d_rho_filter: 3\n",
        "    update_p_bc_by_flow: true\n",
        "  }\n",
        "  thermodynamics {\n",
        "    constant_density {}\n",
        "  }\n",
        "  density: 1.0\n",
        "  kinematic_viscosity: 0.01\n",
        "  boundary_conditions {\n",
        "    name: \"u\"\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 1.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 1\n",
        "      type: BC_TYPE_NEUMANN\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 1\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "  }\n",
        "  boundary_conditions {\n",
        "    name: \"v\"\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 1\n",
        "      type: BC_TYPE_NEUMANN\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 1\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "  }\n",
        "  boundary_conditions {\n",
        "    name: \"w\"\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 0\n",
        "      location: 1\n",
        "      type: BC_TYPE_NEUMANN\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 0\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "    boundary_info {\n",
        "      dim: 1\n",
        "      location: 1\n",
        "      type: BC_TYPE_DIRICHLET\n",
        "      value: 0.0\n",
        "    }\n",
        "  }\n",
        "\"\"\",\n",
        "    grid_params_from_flags=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPGw1oMqsnPM"
      },
      "outputs": [],
      "source": [
        "# Defines the function that initializes state variables.\n",
        "\n",
        "\n",
        "def init_fn_channel(replica_id, coordinates):\n",
        "  \"\"\"Initializes state variables in a channel flow.\"\"\"\n",
        "  del coordinates\n",
        "  nx = params.nx\n",
        "  ny = params.ny\n",
        "  nz = params.nz\n",
        "\n",
        "  return {\n",
        "      'replica_id': replica_id,\n",
        "      'rho': tf.ones((nz, nx, ny), dtype=tf.float32),\n",
        "      'u': tf.ones((nz, nx, ny), dtype=tf.float32),\n",
        "      'v': tf.zeros((nz, nx, ny), dtype=tf.float32),\n",
        "      'w': tf.zeros((nz, nx, ny), dtype=tf.float32),\n",
        "      'p': tf.zeros((nz, nx, ny), dtype=tf.float32),\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EJPqmDYJkJm"
      },
      "source": [
        "# TPU initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCBrOfKjRryM"
      },
      "outputs": [],
      "source": [
        "# Initializes the TPU strategy.\n",
        "computation_shape = np.array([params.cx, params.cy, params.cz])\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "      tpu=''\n",
        "  )  # TPU detection\n",
        "except ValueError:\n",
        "  raise BaseException(\n",
        "      'ERROR: Not connected to a TPU runtime; please see the previous cell in '\n",
        "      'this notebook for instructions!'\n",
        "  )\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "topology = tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "device_assignment, _ = tpu_util.tpu_device_assignment(\n",
        "    computation_shape=computation_shape, tpu_topology=topology\n",
        ")\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(\n",
        "    tpu, device_assignment=device_assignment\n",
        ")\n",
        "logical_coordinates = tpu_util.grid_coordinates(computation_shape).tolist()\n",
        "\n",
        "print('All devices: ', tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMshD0ypJpMK"
      },
      "source": [
        "# Run the simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTenkLxms0Fe"
      },
      "outputs": [],
      "source": [
        "# initializes the simulation.\n",
        "state = driver_tpu.distribute_values(\n",
        "    tpu_strategy,\n",
        "    value_fn=init_fn_channel,\n",
        "    logical_coordinates=logical_coordinates,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKn1z1iDQ2YQ"
      },
      "outputs": [],
      "source": [
        "# The number of time steps to run before restart data files are written. The\n",
        "# completion of these steps is considered as a cycle.\n",
        "num_steps = 100\n",
        "\n",
        "# Runs the simulation for one cycle.\n",
        "step_id = tf.constant(0)\n",
        "kernel_op = get_kernel_fn.ApplyKernelConvOp(params.kernel_size)\n",
        "model = driver._get_model(kernel_op, params)\n",
        "\n",
        "%time\n",
        "state, step_id, _, _ = driver._one_cycle(\n",
        "    strategy=tpu_strategy,\n",
        "    init_state=state,\n",
        "    init_step_id=step_id,\n",
        "    num_steps=num_steps,\n",
        "    params=params,\n",
        "    model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaRaQqgxnvxC"
      },
      "source": [
        "Note that the runtime for the code block above is long. This is due to the JIT compilzation of the TensorFlow graph when a function is called for the first time. The compiled graph will be used for subsequent calls to the same function with the same input type. The actual runtime for 100 steps in this example is\n",
        "around 10 ms (try running the code block below and see how the timing changes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2QQf7wsnpME"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "step_id = driver._local_state_value(tpu_strategy, step_id)[0].numpy()\n",
        "state, step_id, _, _ = driver._one_cycle(\n",
        "    strategy=tpu_strategy,\n",
        "    init_state=state,\n",
        "    init_step_id=step_id,\n",
        "    num_steps=num_steps,\n",
        "    params=params,\n",
        "    model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ebMB2o1HEMS"
      },
      "source": [
        "# Post-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhUEGUqo98I-"
      },
      "outputs": [],
      "source": [
        "# Utility functions for postprocessing.\n",
        "\n",
        "\n",
        "def merge_result(values, coordinates, halo_width):\n",
        "  \"\"\"Merges results from multiple TPU replicas following the topology.\"\"\"\n",
        "  if len(values) != len(coordinates):\n",
        "    raise (\n",
        "        ValueError,\n",
        "        (\n",
        "            'The length of `value` and `coordinates` must equal. Now `values`'\n",
        "            f' has length {len(values)}, but `coordinates` has length'\n",
        "            f' {len(coordinates)}.'\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  # The results are oriented in order z-x-y.\n",
        "  nz, nx, ny = values[0].shape\n",
        "  nz_0, nx_0, ny_0 = [n - 2 * halo_width for n in (nz, nx, ny)]\n",
        "\n",
        "  # The topology is oriented in order x-y-z.\n",
        "  cx, cy, cz = np.array(np.max(coordinates, axis=0)) + 1\n",
        "\n",
        "  # Compute the total size without ghost cells/halos.\n",
        "  shape = [n * c for n, c in zip([nz_0, nx_0, ny_0], [cz, cx, cy])]\n",
        "\n",
        "  result = np.empty(shape, dtype=np.float32)\n",
        "\n",
        "  for replica in range(len(coordinates)):\n",
        "    s = np.roll(\n",
        "        [c * n for c, n in zip(coordinates[replica], (nx_0, ny_0, nz_0))],\n",
        "        shift=1,\n",
        "    )\n",
        "    e = [s_i + n for s_i, n in zip(s, (nz_0, nx_0, ny_0))]\n",
        "    result[s[0] : e[0], s[1] : e[1], s[2] : e[2]] = values[replica][\n",
        "        halo_width : nz_0 + halo_width,\n",
        "        halo_width : nx_0 + halo_width,\n",
        "        halo_width : ny_0 + halo_width,\n",
        "    ]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPlmjPF07-vR"
      },
      "outputs": [],
      "source": [
        "# @title Results visualization\n",
        "varname = 'u'  # @param ['u', 'v', 'w', 'p', 'rho']\n",
        "\n",
        "result = merge_result(\n",
        "    state[varname].values, logical_coordinates, params.halo_width\n",
        ")\n",
        "\n",
        "nx = (params.nx - 2 * params.halo_width) * params.cx\n",
        "ny = (params.ny - 2 * params.halo_width) * params.cy\n",
        "nz = (params.nz - 2 * params.halo_width) * params.cz\n",
        "\n",
        "x = np.linspace(0.0, params.lx, nx)\n",
        "y = np.linspace(0.0, params.ly, ny)\n",
        "z = np.linspace(0.0, params.lz, nz)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "c = ax.contourf(x, y, result[nz // 2, ...].transpose(), cmap='jet', levels=21)\n",
        "fig.colorbar(c)\n",
        "ax.axis('equal')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "last_runtime": {
        "build_target": "//research/simulation/tools:notebook",
        "kind": "private"
      },
      "name": "swirl_lm_public_demo1_channel.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/swirl_lm/example/swirl_lm_public_demo1_channel.ipynb",
          "timestamp": 1663259235977
        },
        {
          "file_id": "1fAoC77ZDdifUu8blRhgyqclULdXAe8yc",
          "timestamp": 1660695462479
        }
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
